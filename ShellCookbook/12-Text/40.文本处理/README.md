# 40.文本处理

## 01.文件内容 清空

01.清空文件内容
cat /dev/null >filename
或 >filename
注意：文件存在会被清空，文件不存在会新建一个名为filename的文件。
---------------------------------------------------------------------------


## 02.文件内容 合并

02.合并文件
通过sort/uniq获取文件内容的交集、合集和不同之处：
假设有a、b两个文本文件，文件本身已经去除了重复内容。下面是效率最高的方法，可以处理任何体积的文件，甚至几个G的文件。
(Sort对内存没有要求，但也许你需要用 -T 参数。)可以试着比较一下，你可以看看如果用Java来处理磁盘上文件的合并，需要用多少行代码。
cat a b | sort | uniq > c   # c 是a和b的合集
cat a b | sort | uniq -d > c   # c 是a和b的交集
cat a b b | sort | uniq -u > c   # c 是a和b的不同
---------------------------------------------------------------------------


## 50.日志处理

01.tail -f /path/to/file.log | sed '/^Finished: SUCCESS$/ q'
当file.log里出现Finished: SUCCESS时候就退出tail，这个命令用于实时监控并过滤log是否出现了某条记录。 
------------------------------------------------------------------------------------------------------------
02.在访问日志中提取出出现频率最高的IP地址
1,2005-02-21 16:49:51,649,221.203.38.132,guest,g
1,2005-02-21 16:49:51,692,60.21.203.65,zhangchangxu,1
1,2005-02-21 16:49:51,779,218.61.139.112,guest,g
awk -F , '{ print $4 }' 1.txt|sort |uniq -c |sort -rn

一个web服务器日志，在某些行上有一些值，比如URL中的acct_id参数。
如果你想统计每个acct_id的所有请求记录：
cat access.log | egrep -o ‘acct_id=[0-9]+’ | cut -d= -f2 | sort | uniq -c | sort -rn
------------------------------------------------------------------------------------------------------------
